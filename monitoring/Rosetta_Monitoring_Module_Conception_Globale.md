# üõ∞Ô∏è Rosetta V2 ‚Äì Conception Globale du Module Monitoring
## Observabilit√©, Pilotage et SRE ‚Äì Dynatrace + ServiceNow + D√©ploiements + Logs (ELISA)

**Auteur / porteur :** Youssef Messaoudi  
**Version :** V0 (cadre cible + feuille de route)  
**Public :** √©quipe Rosetta, architectes, SRE, FinOps, direction IT / programme

---

# 1) Objectifs du module Monitoring

Le module Monitoring de Rosetta V2 vise √† fournir une **vue unifi√©e, gouvern√©e et actionnable** de la sant√© des applications et des plateformes, en s‚Äôappuyant sur des sources existantes (Dynatrace, ServiceNow, ArgoCD/GitLab, ELISA).

## Objectifs principaux
- **Observabilit√©** : m√©triques, √©v√©nements, logs, traces (l√† o√π disponibles)
- **Pilotage SRE** : SLI/SLO, disponibilit√©, erreurs, latence, saturation
- **Pilotage Delivery** : √©tats de d√©ploiement (live + historis√©), DORA metrics
- **Pilotage Run** : incidents, changes, probl√®mes, backlog op√©rationnel
- **Pilotage Direction** : vues consolid√©es, tendances, risques, conformit√© SLO, stabilit√©

## Diff√©rence avec un ‚Äúmonitoring apping‚Äù
Rosetta Monitoring n‚Äôest pas un outil de monitoring bas niveau.  
C‚Äôest une **couche de consolidation** et de **serving** adapt√©e :
- √† Backstage (par application / composant / groupe)
- aux besoins des √©quipes produit/SRE
- au reporting et √† l‚Äôadministration

---

# 2) P√©rim√®tre fonctionnel (ce que couvre Rosetta Monitoring)

## 2.1 Dynatrace ‚Äì m√©triques & events
Rosetta consomme Dynatrace pour :
- Availability / erreurs (taux 5xx, erreurs applicatives)
- Latence (p95/p99, temps de r√©ponse)
- Saturation (CPU/memory, threads, pools‚Ä¶ selon instrumentation)
- Throughput (req/sec)
- Golden Signals (RED / USE)
- Events : incidents Dynatrace (probl√®mes, anomalies, d√©gradations)
- (optionnel) Topology : services, hosts, process groups, k8s entities

## 2.2 ServiceNow ‚Äì incidents / changes
Rosetta consomme ServiceNow pour :
- Incidents (volumes, MTTR, SLA, assignation)
- Changes (calendrier de change, taux d‚Äô√©chec, impact)
- Probl√®mes (si expos√©)
- Liens entre incidents/changes et applications (via CI/CMDB mapping)

## 2.3 D√©ploiements ‚Äì √©tat live + historisation
Rosetta expose :
- **Live** : statut actuel ArgoCD (synced/outofsync, health), dernier pipeline GitLab
- **Historis√©** : s√©ries temporelles d‚Äô√©v√©nements de d√©ploiement
- M√©triques DORA (propos√©es plus bas)

## 2.4 Logs ‚Äì ELISA (Kibana/ELK)
Rosetta ne remplace pas ELISA :
- Il fournit des **liens contextuels** et √©ventuellement des **indicateurs** :
  - volume d‚Äôerreurs logs
  - pr√©sence de patterns ‚Äúerror‚Äù / ‚Äúexception‚Äù
  - liens directs Kibana pr√©filtr√©s (app/env/time range)

---

# 3) Mod√®le d‚Äôint√©gration avec le R√©f√©rentiel Rosetta

## 3.1 Pivot de corr√©lation
Le module monitoring repose sur un pivot stable pour lier :
- **Application / Component (Rosetta)**
- **Entit√©s Dynatrace**
- **CI/CMDB ServiceNow**
- **Applications ArgoCD/GitLab**
- **Index ELISA / logs**

### Strat√©gies possibles de mapping (non exclusives)
1) **Tag standard** dans Rosetta r√©f√©rentiel :
   - `dynatrace.entitySelector` ou `dynatrace.tag`
   - `servicenow.ciId` / `cmdb.ciName`
   - `argocd.appName` / `gitlab.projectId`
   - `elisa.index` / `kibana.queryTemplate`
2) **Mapping table** g√©r√©e par admin monitoring (si donn√©es imparfaites)
3) **D√©couverte automatique** (plus tard) via CMDB / topology

**Recommandation :** d√©marrer avec **tags/pivots d√©claratifs minimaux** + une table de mapping admin, puis automatiser progressivement.

---

# 4) Architecture logicielle cible

## 4.1 Services Rosetta
- **Backstage (1 conteneur)** : UI + backend plugins (consommateur)
- **platform-ref-api** : r√©f√©rentiel (apps/components/groupes + approvals + tags pivots)
- **monitoring-api** : couche serving Monitoring (REST)
- **monitoring-batch** : ingestion/historisation/calcul d‚Äôagr√©gats (CronJob)

## 4.2 Donn√©es
- **Raw/Landing** : Mongo (ou storage brut) pour dumps/exports
- **Data Mart** : PostgreSQL `monitoring_db`
- **Materialized Views** : `mv_*` pour acc√©l√©rer les dashboards

## 4.3 Deux modes de fonctionnement
### A) Live mode (temps r√©el)
- monitoring-api appelle Dynatrace/ArgoCD/GitLab/ELISA √† la vol√©e
- usage : diagnostic instantan√©, statut actuel, navigation

### B) Historis√© mode (batch)
- monitoring-batch ing√®re p√©riodiquement des snapshots
- usage : tendances, reporting, DORA, SLO compliance

---

# 5) Data Mart Monitoring ‚Äì principes (haut niveau)

## 5.1 Niveaux de donn√©es
1) **Facts temporels** (time series) : agr√©g√©s (minute/heure/jour)
2) **Events** : incidents, changements, d√©ploiements
3) **KPI calcul√©s** : SLO compliance, DORA, MTTR, taux incident, etc.

## 5.2 Strat√©gie de granularit√©
Pour rester robuste et performant :
- D√©marrer par des agr√©gats **hourly** ou **daily** par app/component/env
- Ajouter la minute seulement si un cas exigeant appara√Æt

---

# 6) APIs Monitoring ‚Äì contrat fonctionnel (haut niveau)

## 6.1 Endpoints ‚ÄúUser‚Äù (lecture)
- `GET /monitoring/overview?period=last_30d`
  - score sant√© global, top risques, tendances
- `GET /monitoring/applications/{appId}/summary`
  - golden signals, incidents ouverts, SLO, dernier d√©ploiement
- `GET /monitoring/applications/{appId}/timeseries?metric=latency_p95&period=...`
- `GET /monitoring/applications/{appId}/incidents?status=open`
- `GET /monitoring/applications/{appId}/changes?period=...`
- `GET /monitoring/applications/{appId}/deployments?mode=live|history`
- `GET /monitoring/applications/{appId}/logs/link?timeRange=...`

## 6.2 Endpoints ‚ÄúSRE / Direction‚Äù (pilotage)
- `GET /monitoring/kpis/slo?period=quarter&groupBy=domain|group|app`
- `GET /monitoring/kpis/dora?period=last_90d&groupBy=team|app`
- `GET /monitoring/risk/top?metric=error_rate&period=...`

## 6.3 Endpoints ‚ÄúAdmin‚Äù
- `GET /monitoring/admin/batch-runs`
- `GET /monitoring/admin/mappings` (pivots Dynatrace/SN/ELISA)
- `POST /monitoring/admin/mappings` (ajout/correction)
- `POST /monitoring/admin/batch/trigger` (run manuel)
- `GET /monitoring/admin/config/sources` (√©tat des connecteurs)

---

# 7) Vues UI propos√©es (Backstage)

## 7.1 Dashboard global (Direction / Plateforme)
- Score sant√© (rouge/orange/vert) par domaine/entit√©/√©quipe
- Tendances : incidents, disponibilit√©, latence, taux d‚Äôerreur
- Top applications √† risque (SLO breach, incidents r√©currents)
- DORA niveau groupe (si dispo)

## 7.2 Vue Application (onglet Backstage)
- Golden signals : latency, errors, traffic, saturation
- Incidents ouverts + historique court
- Changes (futurs + r√©cents)
- D√©ploiement live (ArgoCD) + dernier pipeline
- Lien ELISA logs pr√©filtr√©
- SLO : objectif vs r√©alis√©

## 7.3 Vue Composant (si vos components sont fins)
- m√™me logique, mais au niveau component (microservice)
- utile pour √©quipes run

## 7.4 Vue SRE / DORA
- Deployment frequency
- Lead time for changes (si donn√©es pipelines dispo)
- Change failure rate (si corr√©lation incidents‚Üîd√©ploiements)
- MTTR

## 7.5 Vue Incidents / Run
- backlog incidents par app / √©quipe
- SLA breach
- heatmap incidents

---

# 8) KPIs SRE/DORA ‚Äì proposition Rosetta

## 8.1 SLO
- Disponibilit√© (uptime)
- Latence p95/p99
- Error budget (budget d‚Äôerreur consomm√©)
- Erreurs 5xx / exceptions

## 8.2 DORA (si historisation delivery)
- Frequency de d√©ploiement (deploy/day)
- Lead time (merge‚Üíprod) (si GitLab/CI)
- Change failure rate (deploy ‚Üí incident li√©)
- MTTR (incident‚Üír√©solution)

**Note :** on peut livrer DORA en 2 √©tapes :
1) ‚ÄúDORA Light‚Äù bas√© sur events d√©ploiement + incidents simples
2) ‚ÄúDORA Full‚Äù avec corr√©lations avanc√©es

---

# 9) Materialized Views ‚Äì besoin & lots (haut niveau)

## Lot A ‚Äì Dashboards
- `mv_monitoring_app_day_summary`
- `mv_monitoring_domain_day_summary`
- `mv_monitoring_top_risks`

## Lot B ‚Äì Incidents/Changes
- `mv_incidents_app_day`
- `mv_changes_app_day`

## Lot C ‚Äì SLO/DORA
- `mv_slo_app_period`
- `mv_dora_app_period`
- `mv_dora_group_period`

**Refresh**
- Daily pour la plupart
- Hourly pour ‚Äúnear-real-time‚Äù si souhait√©

---

# 10) Feuille de route incr√©mentale (propos√©e)

## Phase 0 ‚Äì Fondations (2‚Äì3 semaines)
- D√©finir pivots r√©f√©rentiel (tags)
- Cr√©er monitoring_db (tables de base)
- monitoring-api skeleton + endpoint live ‚Äúsummary‚Äù
- 1 dashboard application minimal

## Phase 1 ‚Äì Historisation minimal (3‚Äì4 semaines)
- monitoring-batch hourly/daily
- ingestion Dynatrace (2‚Äì3 m√©triques cl√©s)
- ingestion incidents ServiceNow
- dashboards tendances + top risks

## Phase 2 ‚Äì D√©ploiements + DORA light (3‚Äì5 semaines)
- statut live ArgoCD
- historisation d√©ploiements
- DORA light (frequency, failure proxy, MTTR)

## Phase 3 ‚Äì SLO avanc√©s + logs ELISA (4‚Äì6 semaines)
- error budget
- lien logs pr√©filtr√© + m√©trique logs
- corr√©lations incidents‚Üîchanges‚Üîdeploy

## Phase 4 ‚Äì Automatisation mapping (√©volutions)
- d√©couverte via CMDB / topology
- r√©duction du d√©claratif

---

# 11) Valeur ‚ÄúDirection‚Äù ‚Äì comment √ßa se vend

Le module Monitoring Rosetta V2 permet :
- Pilotage de la qualit√© de service (SLO)
- Pilotage de la transformation / delivery (DORA)
- Transparence op√©rationnelle (incidents, changes)
- Identification des risques (top apps rouges)
- Gouvernance par √©quipe/domaine (ownership clair)
- Support √† la priorisation investissement (run vs build)

---

# 12) Conclusion

Rosetta Monitoring V2 est une **couche d‚Äôobservabilit√© et de pilotage** :
- adapt√©e au mod√®le Backstage (app/component/group)
- connect√©e aux outils existants (Dynatrace, ServiceNow, ELISA, ArgoCD/GitLab)
- capable de combiner **live** (diagnostic) et **historique** (reporting & KPI)
- livrable incr√©mentalement, avec une trajectoire SRE robuste.

---

## Annexe ‚Äì Hypoth√®ses √† valider (plus tard)
- quelles APIs Dynatrace sont accessibles (metrics v2, problems, entities‚Ä¶)
- quel mapping CI/CMDB ServiceNow existe et sa qualit√©
- quels identifiants ArgoCD/GitLab sont disponibles dans le r√©f√©rentiel
- acc√®s ELISA : lien seulement ou extraction d‚Äôindicateurs
